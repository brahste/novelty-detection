{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "crazy-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from utils import tools, metrics, supported_preprocessing_transforms\n",
    "from modules.cae_base_module import CAEBaseModule\n",
    "from models import supported_models\n",
    "from datasets import supported_datamodules\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mysterious-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configurations and paths to logged models\n",
    "root = Path.cwd() / '..'\n",
    "log_path = root / 'logs' / 'NoveltyMNISTDataModule'\n",
    "paths_to_archived_models = Path(log_path).glob('**/*CAE*/archive*')\n",
    "\n",
    "module_catalog = {}\n",
    "\n",
    "for pth in paths_to_archived_models:\n",
    "    config = tools.load_config(pth / 'configuration.yaml', silent=True)\n",
    "    model_type = pth.parent.name\n",
    "    model_name = pth.name\n",
    "    model_path = next(iter((pth / 'checkpoints').glob('val_loss*')))\n",
    "    \n",
    "    preprocessing_transforms = supported_preprocessing_transforms[config['data-parameters']['preprocessing']]\n",
    "    \n",
    "    datamodule = supported_datamodules[config['experiment-parameters']['datamodule']](\n",
    "        data_transforms=preprocessing_transforms,\n",
    "        **config['data-parameters'])\n",
    "    datamodule.setup('test')\n",
    "\n",
    "    model = supported_models[config['experiment-parameters']['model']](\n",
    "        in_shape=datamodule.data_shape)\n",
    "\n",
    "    module = CAEBaseModule(model, **config['module-parameters'])\n",
    "    \n",
    "    # Load the state_dict into the module architecture\n",
    "    checkpoint = torch.load(model_path)\n",
    "    module.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    if model_type not in module_catalog:\n",
    "        module_catalog[model_type] = {}\n",
    "    module_catalog[model_type][model_name] = module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "regulated-budget",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaselineCAE-v1\n",
      "-----\n",
      "[BATCH 0] Mean score: 0.2508472204208374\n",
      "[BATCH 1] Mean score: 0.2540508508682251\n",
      "[BATCH 2] Mean score: 0.2508455216884613\n",
      "[BATCH 3] Mean score: 0.2571045160293579\n",
      "[BATCH 4] Mean score: 0.2514170706272125\n",
      "[BATCH 5] Mean score: 0.24534529447555542\n",
      "[BATCH 6] Mean score: 0.253703236579895\n",
      "[BATCH 7] Mean score: 0.2512032985687256\n",
      "[BATCH 8] Mean score: 0.252405047416687\n",
      "BaselineCAE-v2\n",
      "-----\n",
      "[BATCH 0] Mean score: 0.2496972531080246\n",
      "[BATCH 1] Mean score: 0.25287121534347534\n",
      "[BATCH 2] Mean score: 0.24970339238643646\n",
      "[BATCH 3] Mean score: 0.25592488050460815\n",
      "[BATCH 4] Mean score: 0.2501794397830963\n",
      "[BATCH 5] Mean score: 0.24416856467723846\n",
      "[BATCH 6] Mean score: 0.25253182649612427\n",
      "[BATCH 7] Mean score: 0.2500435709953308\n",
      "[BATCH 8] Mean score: 0.2512699365615845\n",
      "CompressionCAEHighCapacity-v1\n",
      "-----\n",
      "[BATCH 0] Mean score: 0.28553107380867004\n",
      "[BATCH 1] Mean score: 0.2883744239807129\n",
      "[BATCH 2] Mean score: 0.28409039974212646\n",
      "[BATCH 3] Mean score: 0.29078802466392517\n",
      "[BATCH 4] Mean score: 0.28748732805252075\n",
      "[BATCH 5] Mean score: 0.280193030834198\n",
      "[BATCH 6] Mean score: 0.2875427007675171\n",
      "[BATCH 7] Mean score: 0.2873488664627075\n",
      "[BATCH 8] Mean score: 0.28773513436317444\n",
      "CompressionCAEMidCapacity-v1\n",
      "-----\n",
      "[BATCH 0] Mean score: 0.2949707508087158\n",
      "[BATCH 1] Mean score: 0.29989588260650635\n",
      "[BATCH 2] Mean score: 0.29538464546203613\n",
      "[BATCH 3] Mean score: 0.30158478021621704\n",
      "[BATCH 4] Mean score: 0.29861265420913696\n",
      "[BATCH 5] Mean score: 0.2911257743835449\n",
      "[BATCH 6] Mean score: 0.2976878881454468\n",
      "[BATCH 7] Mean score: 0.2975372076034546\n",
      "[BATCH 8] Mean score: 0.29737433791160583\n",
      "CompressionCAEMidCapacity-v2\n",
      "-----\n",
      "[BATCH 0] Mean score: 0.3066363036632538\n",
      "[BATCH 1] Mean score: 0.3129716217517853\n",
      "[BATCH 2] Mean score: 0.3069831132888794\n",
      "[BATCH 3] Mean score: 0.3137398958206177\n",
      "[BATCH 4] Mean score: 0.3110487163066864\n",
      "[BATCH 5] Mean score: 0.30395084619522095\n",
      "[BATCH 6] Mean score: 0.3107454478740692\n",
      "[BATCH 7] Mean score: 0.31133657693862915\n",
      "[BATCH 8] Mean score: 0.31001371145248413\n"
     ]
    }
   ],
   "source": [
    "# Collect scores and labels for all models into a catalog\n",
    "scores = pd.DataFrame()\n",
    "labels = pd.DataFrame()\n",
    "for model_type, model_variants in module_catalog.items():\n",
    "    \n",
    "    for model_name, module in model_variants.items():\n",
    "        \n",
    "        test_novelty_scores = []\n",
    "        test_novelty_labels = []\n",
    "        short_name = model_type + '-' + model_name.split('_')[1]\n",
    "        print(f'{short_name}\\n-----')\n",
    "        module.model.eval()  # Freeze dropout and batch normalization parameters\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_nb, batch in enumerate(datamodule.test_dataloader()):\n",
    "                \n",
    "                result = module.test_step(batch, batch_nb)\n",
    "\n",
    "                test_novelty_scores.extend(result['scores'].cpu().numpy())\n",
    "                test_novelty_labels.extend(result['labels'].cpu().numpy())\n",
    "                print(f'[BATCH {batch_nb}] Mean score: {result[\"scores\"].mean()}')\n",
    "            scores.loc[:, short_name] = test_novelty_scores\n",
    "            labels.loc[:, short_name] = test_novelty_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().disabled = True\n",
    "plt.figure(figsize=(10,10))\n",
    "for col in scores:\n",
    "    fpr, tpr, thresholds, auc = metrics.roc(scores[col], labels[col])\n",
    "\n",
    "    plt.plot(fpr, tpr, lw=2., label=f'{col} AUC: {auc:.2f}')\n",
    "\n",
    "    print(f'{col} ROC AUC: {auc}')\n",
    "    \n",
    "plt.plot([0., 1.], [0., 1.], 'k:', lw=2, label='Random')\n",
    "plt.legend(fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xlabel('FPR', fontsize=16)\n",
    "plt.ylabel('TPR', fontsize=16)\n",
    "plt.show()\n",
    "print('Random ROC AUC: 0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for col in scores:\n",
    "    pak = metrics.precision_at_k(scores[col], labels[col])\n",
    "\n",
    "    uniques, counts = np.unique(labels[col], return_counts=True)\n",
    "    random = counts.min() / counts.max()\n",
    "\n",
    "    plt.plot(pak, label=f'{col}')\n",
    "    \n",
    "plt.plot([0, 10000], [random, random], 'k:', label='Random')\n",
    "plt.ylim([0., 1.])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c4dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
