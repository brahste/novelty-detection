# experiment-parameters: Contains experiment-level information.
experiment-parameters:

  # name (required, str): The name of the experiment. Typically best to align with the model being used.
  model: 'BaselineVAE'

  # log_dir (required, str): The directory to log the experiments to. Best to keep as 'logs'.
  log_dir: 'logs'

  # datamodule (required, str): The datamodule (or datagenerator) to use for your experiment.
  datamodule: 'CuriosityDataModule'

  # patience (optional, int, default=5) Determines when to conduct early stopping based on lowest validation loss.
  # Set to null for default.
  patience: null


# data-parameters: Contains information used for preparing and loading the data.
data-parameters:

  # root_data_path (required, str): Path to the common directory node immediately before training and testing branches.
  ###root_data_path: '/home/fenrir/Documents/Datasets/NoveltyMNIST'
  root_data_path: '/home/brahste/Datasets/MartianCuriosity'

  # train_fraction (optional, float, default=0.8): Fraction of training samples to be used for
  # training, the rest will be used for validation. Set to null for default.
  train_fraction: 0.9

  # batch_size (optional, int, default=8): Number of samples to be used in each iteration.
  # Set to null for default.
  batch_size: 128

  # preprocessing (required, str): The preprocessing transforms applied to each image before being imported for
  # training, validation, or testing. Options are found in utils/__init__.py.
  preprocessing: 'CuriosityPreprocessing'


# module-parameters: Contains hyperparameters used when training the model.
module-parameters:

  # learning_rate (optional, float, default='auto'): Scaling factor for gradient descent step.
  # Set to null for default.
  learning_rate: null

  # weight_decay_coefficient (optional, float, default=0.01): Scaling factor for loss regularization term.
  # Set to null for default.
  weight_decay_coefficient: 0.01

  # latent_dims (required, int): Number of dimensions to use in the latent space of the VAE
  latent_nodes: 32
